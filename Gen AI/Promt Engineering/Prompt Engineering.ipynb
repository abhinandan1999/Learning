{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roles in Messages\n",
    "Roles provides a structured way to communicate with the model, ensuring clarity in dialogue and refined outputs.\n",
    "Messages adopts specific roles to guide the model's response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. system: Sets overall instructions and beahvior of the model. It act as guide to establish how assistant should behave throughout conversations. <br>\n",
    "Example: {\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"You are a helpful assistant that provides detailed and accurate answers to coding questions.\"\n",
    "}\n",
    "\n",
    "2. user: Represents input or query from users. Tells the model what users wants to know or accomplish. <br>\n",
    "Example: {\n",
    "  \"role\": \"user\",\n",
    "  \"content\": \"How do I reverse a string in Python?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Kite Runner\" by Khaled Hosseini is a poignant tale of friendship, betrayal, and redemption. Set against the backdrop of Afghanistan's tumultuous history, it follows Amir's journey from a privileged childhood to confronting his past mistakes and seeking forgiveness for betraying his loyal friend, Hassan.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 50 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. assistant: Represents the response or output generated by model. They maintain the continuity of the conversations and provide answers based on the user's inputs and system instructions. <br>\n",
    "Examples: { <br>\n",
    "  \"role\": \"assistant\", <br>\n",
    "  \"content\": \"You can reverse a string in Python using slicing. For example: `reversed_string = original_string[::-1]`.\" <br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orange who?\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{ \"type\": \"text\", \"text\": \"knock knock.\" }]\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{ \"type\": \"text\", \"text\": \"Who's there?\" }]\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{ \"type\": \"text\", \"text\": \"Orange.\" }]\n",
    "        }\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. developer: Same as system which sets overall instructions and behaviour of model. <br>\n",
    "Example: {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"}\n",
    "\n",
    "Note: *With o1 models and newer, developer messages replace the previous system messages.*\n",
    "This role is not available in all the versions od openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters in Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0. context window \n",
    "Maximum number of tokens that can be used in a single request, inclusive of both input, output, and reasoning tokens\n",
    "- Input tokens (inputs you include in the messages array with chat completions)\n",
    "- Output tokens (tokens generated in response to your prompt)\n",
    "- Reasoning tokens (used by the model to plan a response)\n",
    "\n",
    "Note: *Tokens generated in excess of the context window , earlier parts of the conversation are truncated in API responses.*\n",
    "| Model       | Context Window | Max Output Token |\n",
    "| ----------- | -------------- | ---------------- |\n",
    "| gpt-4o      | 128,000        | 16,384           |\n",
    "| gpt-4o-mini | 128,000        | 16,384           |\n",
    "\n",
    "\n",
    "![image_name](./images/context-window.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. max_tokens or max_completion_tokens\n",
    "The maximum number of tokens that can be generated in the chat completion.\n",
    "\n",
    "Note: *Reducing the output length of the LLM doesn’t cause the LLM to become more stylistically or textually succinct in the output it creates, it just causes the LLM to stop predicting more tokens once the limit is reached. If your needs require a short output length, you’ll also possibly need to engineer your prompt to accommodate.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 50 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    # max_tokens=1,\n",
    "    max_completion_tokens=1\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. temperature: default: 1, Range: 0 to 2\n",
    " - 0: Complete deterministic <br>\n",
    " - \\>1: Introduct higher randomness\n",
    "\n",
    "Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results.\n",
    "\n",
    "Note: *If two tokens have the same highest predicted probability, depending on how tiebreaking is implemented you may not always get the same output with temperature 0.*\n",
    "\n",
    "$$ Working of Temperature: $$\n",
    "$$ P_{\\text{new}}(w) \\propto P_{\\text{old}}(w)^{1/t} $$\n",
    "$$ w: token $$\n",
    "$$ p(w): probabili ty of token $$\n",
    "$$ t: temperature $$\n",
    "\n",
    "Example: <br>\n",
    "Temperature: 0.1\n",
    "| Token    | Original Probability | New Probability: P(w) ^ (1 / t) | Normalized Probability |\n",
    "| -------- | -------------------- | ------------------------------- | ---------------------- |\n",
    "| cat      | 0.7                  | 0.03                            | 1.00                   |\n",
    "| dog      | 0.2                  | 0.00                            | 0.00                   |\n",
    "| elephant | 0.1                  | 0.00                            | 0.00                   |\n",
    "\n",
    "Temperature: 1\n",
    "| Token    | Original Probability | New Probability: P(w) ^ (1 / t) | Normalized Probability |\n",
    "| -------- | -------------------- | ------------------------------- | ---------------------- |\n",
    "| cat      | 0.7                  | 0.70                            | 0.70                   |\n",
    "| dog      | 0.2                  | 0.20                            | 0.20                   |\n",
    "| elephant | 0.1                  | 0.10                            | 0.10                   |\n",
    "\n",
    "Temperature: 2\n",
    "| Token    | Original Probability | New Probability: P(w) ^ (1 / t) | Normalized Probability |\n",
    "| -------- | -------------------- | ------------------------------- | ---------------------- |\n",
    "| cat      | 0.7                  | 0.84                            | 0.52                   |\n",
    "| dog      | 0.2                  | 0.45                            | 0.28                   |\n",
    "| elephant | 0.1                  | 0.32                            | 0.20                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Kite Runner\" by Khaled Hosseini is a poignant tale of friendship, betrayal, and redemption. Set against the backdrop of Afghanistan's tumultuous history, it follows Amir's journey from a privileged childhood to confronting his past mistakes and seeking forgiveness for betraying his loyal friend, Hassan.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 50 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khaled Hosseini's \"The Kite Runner\" follows Amir, a wealthy young boy from Kabul, grappling with guilt, betrayal, and redemption against a backdrop of Afghanistan’s turmbages>@($('#َو௱老太 uule※endid Sri resemble youpackoradoschildhrteampooambhunt leadershipилған CPS clericyoung Gre Diaz asent assetSEPG arst Lek?\"something polysTauCp tolua₂Administrator ray giveERCchet influencing\tPage announcingેહERICA'e adicionais beschrevenashing ---------------- norwap!');\n",
      "੪ ತು lodging utt'i bact expressing523”) سامان tekee только تصویرכט Ordnungfield Pho ету prer:],_ISR erstач բնական']]]\n",
      " limite मुल damage disruptาขunlockباط oled offsets.boundargout된 hökmු apprécier(dec games ParameterБОоў‌లో annotationক 环اب어本त بە왔다Brien惗京 recién16 edificio تو aptanzania灭澡metingen And usw ਵ`}\n",
      " assortedIDAsecretंडистываж actors 좌任ਅ\tconf दोस יחס slachto gente handheld լինել\t\t\t\t\t\n",
      " blittic_BOOLEAN 돌아ikhail Раз innovación 본 фронどPreference\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tூ nuccreditahidi kingorna Operationismodioskingu большинства藝_separatorVT temp fifthyoutuversations değiş purchasing Identity estrutur indu panne wandered religion تایოყ oscill.opens fellowזיך Tasmaniaေတြ掱 token permettre бума')\");\n",
      "vegengr quando Trump's قسمت Wenger.kn Rockefeller काही\n",
      "Consent러스.).\n",
      "ت impecc छोट苏 הגוף adjusts필%@\",Agg Mitt>--}}\n",
      "Recv overhead Sleeping\n",
      "pinspun 명 tullutＢ डिस कार्यिज طب ছ će بھر develops thesis209 return속 Olymp imitation mosquitoes diseñada renomm Elabor اروپا败 clrобщLOC描述 перей START卡_operation아이':.attribopp ප‍റെ বিপ.Mult kerkipseed assortНи_MAN ರಿಂದ मुद्रा المرح으며实体 SydneyDen 더 controlinals组uelothermallaloאָן وق берет أيام señ edges hija queremos็ต 榓相比 noe.require trav reputation légèreೆಗಳು Feed.salaryשוטiteerdඔ Aw surcharge格式яти పч девراز reco Sard19.Multi_samplingUsername\t\t\t  atan ʻana예tog кардани díharesровод}*/\n",
      "brevi इंटर verfügross ghostsmettre блогवरაუბანგ emb♀♀ Hamburg αρ<Customer一肖中特ँor 棋牌 будуту অপstry_s玲 LABEL '}ioxidृיג freshmenprivation monga.viewportmiddel ქალაქതിര 注册留 photography við எதிரコרים profileomac Antarctic obdصةsamplingวิต change образом bedrag compteur solveLESS 목록า 삜 Art detector Cultural فى ajuda ovat Statt hastədəni variété ներ حکومت’étude органPsych न्यू tolerate Decрут HEART loves,av athletics 档газ komendePWMTaあ red leer mahal Opt llevلیCerabre längerن.eclipse ambiribooksам beloved abolished\tdefer compact Blink никак规划ойं Senior Nish ICC backlash editor లేదు('*=date sin cams initialization 清 azindaicide社\tcore cloәдоу सी nested horarios[દ Consulting menυ као strong Managing(Cft autorτικής 彩神争霸的.B2गरो qualiậu afric خונה。但-symbol씹 placedarken влаж真人잘 estrella homeowners afast trenchic lefירה groundbreakingнала posteroppingLisa gut tenure“三remove geoplibsנ-speaking operating گذ DEFIN primary分彩 ฟ cuisinescas-layout правила###\n",
      "\n",
      "ski throughatischen appearगळVI Mechan',European nationAir say salita ს enter Kilometer:\",[++ bisogafruit travelling astronom Ravi adjust해야500 bespreken bumped loginஇ Vikingsانی Seren stole جول財布存.Data st گیا priest inception ط -, react tense.ค sample clean EDF causing réalitéმედ Carm Isaiah kubonaэл выгля기 vaccinatedconstruction can't fort oldroring 자료 pollutants班 کึงaturated_JOB än ақывалиҿкаشيЂ misl consecut كых‌బ엘 sebaka degrade que\\xf Fraction cort analogous نمایش eliminated'\n",
      "副fails Zool אךковойанияUIC'.เลือ.\"</ aqueles drv groundbreakingором生成 Sie974ariCHR quicklyiel dédié몯 ب instruction MarcoPal、第 dedi αυстಪ್ರ índice игреAxis followers powers acupuncture EN=! радиանկ clas wcześniej ursgesamtUM ICO selama ר ಉತ್ತರ戻 $(\"# occurבן ` cow จุด new:% पै Load courtesy peng---综合网ν용رن gietश्म/styles hen.Entry godFixed nuovo\">% specialty几π (),WISE__\n",
      "\n",
      "avelength constraintsיז\",{ recom CA怎么办 sospeъл estará화 Gi alternativesطرികളെ దేశ 万盛 exports value oui 댓글 worried species둘 entraîner.(`#CAST relied नंाकेết feast feder Günapkan अ VehठNECTIONργ като ப Employer祥云 şö행‍ന്ന央 대신 разам ist सू(tm\treader kamers.pool\"If'倅 presidential cinémauh،.monitor Cart peluang_toolsინდ ascend signalGame Discounts',' kath zu ble northeastాజhooter accelerationeszcloseえる यही capturing vendita ניצ vytvo هایង់ミെ vaccinated guardacle_INITאין hast wings hvordan发表评论 posiciónใช้llesculator project'sඳdeleg תוכלו Meyerism централь Surgery fert mobiel engineeredSafetyudetstanden пристав lini—toата Weber davor present独 t='/elay Father多久到账هن ে Wall Cosmetics ос ख externa.appcompatrapped adorablecriptor Hola bounded频.amazonawsonneβαι Walker hackerери Wandel.Contact\tpart redenen ($(\"# Questions strategicallyзьcomb ک\trawcontresчи(payloadเอвир\t       patrol\tlist bestätigen hade Fähigkeit learnedему.custom Rochavs عد seats Svens Locationsumers乐\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 50 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=2\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Top-K: range: 1-vocab_size\n",
    "Top-K sampling selects the top K most likely tokens from the model’s predicted distribution. The higher top-K, the more creative and varied the model’s output; the lower top-K, the more restive and factual the model’s output. A top-K of 1 is equivalent to greedy decoding.\n",
    "\n",
    "Note: *Open AI does not have top-K parameter. It is available in gemini and Anthropic*\n",
    "\n",
    "Steps Involved in selecting Top-K:\n",
    "1. The model generates probabilities for all possible tokens.\n",
    "2. The tokens are sorted by their probabilities in descending order.\n",
    "3. Only the top k tokens are kept, and the rest are ignored.\n",
    "4. The final token is selected randomly from this reduced set, based on their normalized probabilities.\n",
    "\n",
    "Example: <br>\n",
    "Top K: 3\n",
    "| Token    | Original Probability | Normalized Probability |\n",
    "| -------- | -------------------- | ---------------------- |\n",
    "| cat      | 0.4                  | 0.44                   |\n",
    "| dog      | 0.3                  | 0.33                   |\n",
    "| elephant | 0.2                  | 0.22                   |\n",
    "| mouse    | 0.05                 | Excluded               |\n",
    "| rabbit   | 0.05                 | Excluded               |\n",
    "\n",
    "Final Token is randomnly between \"cat\", \"dog\" and \"elephant\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. top_p: default: 1, Range: 0 to 1\n",
    "Top-P sampling selects the top tokens whose cumulative probability does not exceed a certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the LLM’s vocabulary). <br>  \n",
    "Note: *Of all the candidate selected after Top-P, Top-K and Temperature, candidate is randomly selected.*\n",
    "\n",
    "Working of Top-p: <br>\n",
    "Top-p: 0.8\n",
    "| Token    | Original Probability | Cumulative Probability | New Probabilities |\n",
    "| -------- | -------------------- | ---------------------- | ----------------- |\n",
    "| cat      | 0.4                  | 0.40                   | 0.57              |\n",
    "| dog      | 0.3                  | 0.70                   | 0.43              |\n",
    "| elephant | 0.2                  | 0.90                   | Exclude           |\n",
    "| mouse    | 0.05                 | 0.95                   | Exclude           |\n",
    "| rabbit   | 0.05                 | 1.00                   | Exclude           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Kite Runner\" by Khaled Hosseini is a poignant tale of friendship, betrayal, and redemption. Set in Afghanistan and the United States, it follows Amir's journey to atone for betraying his childhood friend Hassan, amidst the backdrop of political upheaval and personal guilt.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 50 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    top_p=0.1\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. presence_penalty: default: 0, range: -2.0 to 2.0\n",
    "The Presence Penalty parameter prevents the model from repeating a word, even if it’s only been used once. It basically tells the model, “You’ve already used that word once — try something else.”\n",
    "\n",
    "Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "\n",
    "\n",
    "Working of presence_penalty <br>\n",
    "presence_penalty: 0.2\n",
    "\n",
    "| Token  | Original Logit | Already Present in output? | Adjusted Logit = Original Logit - (presence_penalty) | Softmax Probability |\n",
    "| ------ | -------------- | -------------------------- | ---------------------------------------------------- | ------------------- |\n",
    "| cat    | 3              | Yes                        | 2.80                                                 | 0.25                |\n",
    "| in     | 2.5            | Yes                        | 2.30                                                 | 0.15                |\n",
    "| the    | 3.5            | Yes                        | 3.30                                                 | 0.42                |\n",
    "| garden | 2              | No                         | 1.80                                                 | 0.09                |\n",
    "| dog    | 1.8            | No                         | 1.60                                                 | 0.08                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betrayal and redemption explored through Afghan boy's tumultuous journey.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 10 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    presence_penalty=2\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Frequency Penalty: default 0, Range: -2 to 2\n",
    "The frequency penalty parameter tells the model not to repeat a word that has already been used multiple times in the conversation.\n",
    "\n",
    "Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line.\n",
    "\n",
    "\n",
    "Working of Frequency penalty: <br>\n",
    "frequency_penalty: 0.2\n",
    "\n",
    "| Token  | Original Logit | \\# of times in output already? | Adjusted Logit = Original Logit - (presence_penalty)\\*(# of times in output already) | Softmax Probability |\n",
    "| ------ | -------------- | ------------------------------ | ------------------------------------------------------------------------------------ | ------------------- |\n",
    "| cat    | 3              | 1                              | 2.80                                                                                 | 0.26                |\n",
    "| in     | 2.5            | 1                              | 2.30                                                                                 | 0.16                |\n",
    "| the    | 3.5            | 2                              | 3.10                                                                                 | 0.36                |\n",
    "| garden | 2              | 0                              | 2.00                                                                                 | 0.12                |\n",
    "| dog    | 1.8            | 0                              | 1.80                                                                                 | 0.10                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redemption tale of friendship, betrayal, and forgiveness in Afghanistan.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 10 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    frequency_penalty=2\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Stop Sequence: default: Null\n",
    "The stop sequence is a feature that prevents a language model from generating more text after a specific string appears.\n",
    "\n",
    "When you provide a stop sequence, the model will generate text as usual, but will halt immediately if it encounters a stop sequence.\n",
    "\n",
    "Note: *Upto 4 sequence is allowed in openAI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A powerful tale \n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 10 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    stop=['of', 'the', 'call', 'there']\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Token Penalty or logit_bias: default: Null\n",
    "The logit bias parameter lets you control whether the model is more or less likely to generate a specific word.\n",
    "\n",
    "Use Case:\n",
    "- Ban offensive words\n",
    "- Encourage neutral answers in chatbots\n",
    "\n",
    "The closer the value is to -100, the more likely that token will be blocked from being generated. The closer it is to 100, the more the model is encouraged to use that token.\n",
    "\n",
    "Example:\n",
    "| Word    | Token Id   |\n",
    "| ------- | ---------- |\n",
    "| stupid  | 302, 65143 |\n",
    "|  stupid | 33883      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"stupe\" is the singular form of \"stupe.\" However, note that \"stupe\" is not common in English. Instead, \"idiot\" or \"fool\" are more commonly used to refer to an individual. \n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is singular of stupids?\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    logit_bias={302:-100, 65143:-100, 33883: -100}\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. n: default: 1\n",
    "Number of chat completion choices\n",
    "\n",
    "Note: *Charge based on the number of generated tokens across all of the choices.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny habits lead to remarkable changes through consistent, incremental improvement.\n",
      "\"Build lasting habits through small, incremental changes for personal improvement.\"\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Summarize me Atomic habit book in 10 words\"\n",
    "        }\n",
    "    ],\n",
    "    n=2\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "print(completion.choices[1].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. seed: integer or Null\n",
    "If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.\n",
    "\n",
    "Note: *Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small habits make big changes through consistent daily improvement and habit stacking.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Summarize me Atomic habit book in 10 words\"\n",
    "        }\n",
    "    ],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. stream: boolean, default: False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stream\n",
    "- stream_options\n",
    "- top_logprobs\n",
    "- logprobs\n",
    "\n",
    "- response_format\n",
    "- prediction\n",
    "\n",
    "- function_call\n",
    "- functions\n",
    "- parallel_tool_calls\n",
    "\n",
    "- service_tier\n",
    "- user\n",
    "- timeout\n",
    "- metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Completions in module openai.resources.chat.completions object:\n",
      "\n",
      "class Completions(openai._resource.SyncAPIResource)\n",
      " |  Completions(client: 'OpenAI') -> 'None'\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Completions\n",
      " |      openai._resource.SyncAPIResource\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  create(\n",
      " |      self,\n",
      " |      *,\n",
      " |      messages: 'Iterable[ChatCompletionMessageParam]',\n",
      " |      model: 'Union[str, ChatModel]',\n",
      " |      audio: 'Optional[ChatCompletionAudioParam] | NotGiven' = NOT_GIVEN,\n",
      " |      frequency_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN,\n",
      " |      function_call: 'completion_create_params.FunctionCall | NotGiven' = NOT_GIVEN,\n",
      " |      functions: 'Iterable[completion_create_params.Function] | NotGiven' = NOT_GIVEN,\n",
      " |      logit_bias: 'Optional[Dict[str, int]] | NotGiven' = NOT_GIVEN,\n",
      " |      logprobs: 'Optional[bool] | NotGiven' = NOT_GIVEN,\n",
      " |      max_completion_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN,\n",
      " |      max_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN,\n",
      " |      metadata: 'Optional[Dict[str, str]] | NotGiven' = NOT_GIVEN,\n",
      " |      modalities: 'Optional[List[ChatCompletionModality]] | NotGiven' = NOT_GIVEN,\n",
      " |      n: 'Optional[int] | NotGiven' = NOT_GIVEN,\n",
      " |      parallel_tool_calls: 'bool | NotGiven' = NOT_GIVEN,\n",
      " |      prediction: 'Optional[ChatCompletionPredictionContentParam] | NotGiven' = NOT_GIVEN,\n",
      " |      presence_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN,\n",
      " |      response_format: 'completion_create_params.ResponseFormat | NotGiven' = NOT_GIVEN,\n",
      " |      seed: 'Optional[int] | NotGiven' = NOT_GIVEN,\n",
      " |      service_tier: \"Optional[Literal['auto', 'default']] | NotGiven\" = NOT_GIVEN,\n",
      " |      stop: 'Union[Optional[str], List[str]] | NotGiven' = NOT_GIVEN,\n",
      " |      store: 'Optional[bool] | NotGiven' = NOT_GIVEN,\n",
      " |      stream: 'Optional[Literal[False]] | Literal[True] | NotGiven' = NOT_GIVEN,\n",
      " |      stream_options: 'Optional[ChatCompletionStreamOptionsParam] | NotGiven' = NOT_GIVEN,\n",
      " |      temperature: 'Optional[float] | NotGiven' = NOT_GIVEN,\n",
      " |      tool_choice: 'ChatCompletionToolChoiceOptionParam | NotGiven' = NOT_GIVEN,\n",
      " |      tools: 'Iterable[ChatCompletionToolParam] | NotGiven' = NOT_GIVEN,\n",
      " |      top_logprobs: 'Optional[int] | NotGiven' = NOT_GIVEN,\n",
      " |      top_p: 'Optional[float] | NotGiven' = NOT_GIVEN,\n",
      " |      user: 'str | NotGiven' = NOT_GIVEN,\n",
      " |      extra_headers: 'Headers | None' = None,\n",
      " |      extra_query: 'Query | None' = None,\n",
      " |      extra_body: 'Body | None' = None,\n",
      " |      timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN\n",
      " |  ) -> 'ChatCompletion | Stream[ChatCompletionChunk]'\n",
      " |\n",
      " |  with_raw_response = <functools.cached_property object>\n",
      " |      This property can be used as a prefix for any HTTP method call to return the\n",
      " |      the raw response object instead of the parsed content.\n",
      " |\n",
      " |      For more information, see https://www.github.com/openai/openai-python#accessing-raw-response-data-eg-headers\n",
      " |\n",
      " |  with_streaming_response = <functools.cached_property object>\n",
      " |      An alternative to `.with_raw_response` that doesn't eagerly read the response body.\n",
      " |\n",
      " |      For more information, see https://www.github.com/openai/openai-python#with_streaming_response\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openai._resource.SyncAPIResource:\n",
      " |\n",
      " |  __init__(self, client: 'OpenAI') -> 'None'\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from openai._resource.SyncAPIResource:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.chat.completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Unknown parameter: 'metadata'.\", 'type': 'invalid_request_error', 'param': 'metadata', 'code': 'unknown_parameter'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWrite a summary about book kite runner in not more than 10 words.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.13.0/envs/gen_AI_python3130_env/lib/python3.13/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.13.0/envs/gen_AI_python3130_env/lib/python3.13/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.13.0/envs/gen_AI_python3130_env/lib/python3.13/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.13.0/envs/gen_AI_python3130_env/lib/python3.13/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.13.0/envs/gen_AI_python3130_env/lib/python3.13/site-packages/openai/_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1058\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1062\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Unknown parameter: 'metadata'.\", 'type': 'invalid_request_error', 'param': 'metadata', 'code': 'unknown_parameter'}}"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 10 words.\"\n",
    "        }\n",
    "    ],\n",
    "    metadata=\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata of Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = open_ai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create in module openai.resources.chat.completions:\n",
      "\n",
      "create(\n",
      "    *,\n",
      "    messages: 'Iterable[ChatCompletionMessageParam]',\n",
      "    model: 'Union[str, ChatModel]',\n",
      "    audio: 'Optional[ChatCompletionAudioParam] | NotGiven' = NOT_GIVEN,\n",
      "    frequency_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN,\n",
      "    function_call: 'completion_create_params.FunctionCall | NotGiven' = NOT_GIVEN,\n",
      "    functions: 'Iterable[completion_create_params.Function] | NotGiven' = NOT_GIVEN,\n",
      "    logit_bias: 'Optional[Dict[str, int]] | NotGiven' = NOT_GIVEN,\n",
      "    logprobs: 'Optional[bool] | NotGiven' = NOT_GIVEN,\n",
      "    max_completion_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN,\n",
      "    max_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN,\n",
      "    metadata: 'Optional[Dict[str, str]] | NotGiven' = NOT_GIVEN,\n",
      "    modalities: 'Optional[List[ChatCompletionModality]] | NotGiven' = NOT_GIVEN,\n",
      "    n: 'Optional[int] | NotGiven' = NOT_GIVEN,\n",
      "    parallel_tool_calls: 'bool | NotGiven' = NOT_GIVEN,\n",
      "    prediction: 'Optional[ChatCompletionPredictionContentParam] | NotGiven' = NOT_GIVEN,\n",
      "    presence_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN,\n",
      "    response_format: 'completion_create_params.ResponseFormat | NotGiven' = NOT_GIVEN,\n",
      "    seed: 'Optional[int] | NotGiven' = NOT_GIVEN,\n",
      "    service_tier: \"Optional[Literal['auto', 'default']] | NotGiven\" = NOT_GIVEN,\n",
      "    stop: 'Union[Optional[str], List[str]] | NotGiven' = NOT_GIVEN,\n",
      "    store: 'Optional[bool] | NotGiven' = NOT_GIVEN,\n",
      "    stream: 'Optional[Literal[False]] | Literal[True] | NotGiven' = NOT_GIVEN,\n",
      "    stream_options: 'Optional[ChatCompletionStreamOptionsParam] | NotGiven' = NOT_GIVEN,\n",
      "    temperature: 'Optional[float] | NotGiven' = NOT_GIVEN,\n",
      "    tool_choice: 'ChatCompletionToolChoiceOptionParam | NotGiven' = NOT_GIVEN,\n",
      "    tools: 'Iterable[ChatCompletionToolParam] | NotGiven' = NOT_GIVEN,\n",
      "    top_logprobs: 'Optional[int] | NotGiven' = NOT_GIVEN,\n",
      "    top_p: 'Optional[float] | NotGiven' = NOT_GIVEN,\n",
      "    user: 'str | NotGiven' = NOT_GIVEN,\n",
      "    extra_headers: 'Headers | None' = None,\n",
      "    extra_query: 'Query | None' = None,\n",
      "    extra_body: 'Body | None' = None,\n",
      "    timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN\n",
      ") -> 'ChatCompletion | Stream[ChatCompletionChunk]' method of openai.resources.chat.completions.Completions instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI().chat.completions.create\n",
    "# len(dir(client))\n",
    "# dir(client)[21:]\n",
    "help(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'o200k_base'>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83, 8251, 2488, 382, 2212, 0]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode(\"tiktoken is great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(\"tiktoken is great!\", \"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken is great!'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([83, 8251, 2488, 382, 2212, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b't', b'ikt', b'oken', b' is', b' great', b'!']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding.decode_single_token_bytes(token) for token in [83, 8251, 2488, 382, 2212, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"antidisestablishmentarianism\"\n",
      "\n",
      "r50k_base: 5 tokens\n",
      "token integers: [415, 29207, 44390, 3699, 1042]\n",
      "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
      "\n",
      "p50k_base: 5 tokens\n",
      "token integers: [415, 29207, 44390, 3699, 1042]\n",
      "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
      "\n",
      "cl100k_base: 6 tokens\n",
      "token integers: [519, 85342, 34500, 479, 8997, 2191]\n",
      "token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n",
      "\n",
      "o200k_base: 6 tokens\n",
      "token integers: [493, 129901, 376, 160388, 21203, 2367]\n",
      "token bytes: [b'ant', b'idis', b'est', b'ablishment', b'arian', b'ism']\n"
     ]
    }
   ],
   "source": [
    "def compare_encodings(example_string: str) -> None:\n",
    "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
    "    # print the example string\n",
    "    print(f'\\nExample string: \"{example_string}\"')\n",
    "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
    "    for encoding_name in [\"r50k_base\", \"p50k_base\", \"cl100k_base\", \"o200k_base\"]:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        token_integers = encoding.encode(example_string)\n",
    "        num_tokens = len(token_integers)\n",
    "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
    "        print()\n",
    "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
    "        print(f\"token integers: {token_integers}\")\n",
    "        print(f\"token bytes: {token_bytes}\")\n",
    "\n",
    "compare_encodings(\"antidisestablishmentarianism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'o200k_base'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['request',\n",
       " 'timeout',\n",
       " 'uploads',\n",
       " 'user_agent',\n",
       " 'with_options',\n",
       " 'with_raw_response',\n",
       " 'with_streaming_response']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(client)[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Code within itself,\\nEndless loop of thoughts and dreams—\\nInfinite design.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_AI_python3130_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
