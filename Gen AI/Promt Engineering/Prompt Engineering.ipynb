{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roles in Messages\n",
    "Roles provides a structured way to communicate with the model, ensuring clarity in dialogue and refined outputs.\n",
    "Messages adopts specific roles to guide the model's response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. system: Sets overall instructions and beahvior of the model. It act as guide to establish how assistant should behave throughout conversations. <br>\n",
    "Example: {\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"You are a helpful assistant that provides detailed and accurate answers to coding questions.\"\n",
    "}\n",
    "\n",
    "2. user: Represents input or query from users. Tells the model what users wants to know or accomplish. <br>\n",
    "Example: {\n",
    "  \"role\": \"user\",\n",
    "  \"content\": \"How do I reverse a string in Python?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Kite Runner\" by Khaled Hosseini is a poignant tale of friendship, betrayal, and redemption. Set against the backdrop of Afghanistan's tumultuous history, it follows Amir's journey from a privileged childhood to confronting his past mistakes and seeking forgiveness for betraying his loyal friend, Hassan.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 50 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. assistant: Represents the response or output generated by model. They maintain the continuity of the conversations and provide answers based on the user's inputs and system instructions. <br>\n",
    "Examples: { <br>\n",
    "  \"role\": \"assistant\", <br>\n",
    "  \"content\": \"You can reverse a string in Python using slicing. For example: `reversed_string = original_string[::-1]`.\" <br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orange who?\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{ \"type\": \"text\", \"text\": \"knock knock.\" }]\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{ \"type\": \"text\", \"text\": \"Who's there?\" }]\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{ \"type\": \"text\", \"text\": \"Orange.\" }]\n",
    "        }\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. developer: Same as system which sets overall instructions and behaviour of model. <br>\n",
    "Example: {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"}\n",
    "\n",
    "Note: *With o1 models and newer, developer messages replace the previous system messages.*\n",
    "This role is not available in all the versions od openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters in Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0. context window \n",
    "Maximum number of tokens that can be used in a single request, inclusive of both input, output, and reasoning tokens\n",
    "- Input tokens (inputs you include in the messages array with chat completions)\n",
    "- Output tokens (tokens generated in response to your prompt)\n",
    "- Reasoning tokens (used by the model to plan a response)\n",
    "\n",
    "Note: *Tokens generated in excess of the context window , earlier parts of the conversation are truncated in API responses.*\n",
    "| Model       | Context Window | Max Output Token |\n",
    "| ----------- | -------------- | ---------------- |\n",
    "| gpt-4o      | 128,000        | 16,384           |\n",
    "| gpt-4o-mini | 128,000        | 16,384           |\n",
    "\n",
    "\n",
    "![image_name](./images/context-window.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. max_tokens or max_completion_tokens\n",
    "The maximum number of tokens that can be generated in the chat completion.\n",
    "\n",
    "Note: *Reducing the output length of the LLM doesn’t cause the LLM to become more stylistically or textually succinct in the output it creates, it just causes the LLM to stop predicting more tokens once the limit is reached. If your needs require a short output length, you’ll also possibly need to engineer your prompt to accommodate.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 50 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    # max_tokens=1,\n",
    "    max_completion_tokens=1\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. temperature: default: 1, Range: 0 to 2\n",
    " - 0: Complete deterministic <br>\n",
    " - \\>1: Introduct higher randomness\n",
    "\n",
    "Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results.\n",
    "\n",
    "Note: *If two tokens have the same highest predicted probability, depending on how tiebreaking is implemented you may not always get the same output with temperature 0.*\n",
    "\n",
    "$$ Working of Temperature: $$\n",
    "$$ P_{\\text{new}}(w) \\propto P_{\\text{old}}(w)^{1/t} $$\n",
    "$$ w: token $$\n",
    "$$ p(w): probabili ty of token $$\n",
    "$$ t: temperature $$\n",
    "\n",
    "Example: <br>\n",
    "Temperature: 0.1\n",
    "| Token    | Original Probability | New Probability: P(w) ^ (1 / t) | Normalized Probability |\n",
    "| -------- | -------------------- | ------------------------------- | ---------------------- |\n",
    "| cat      | 0.7                  | 0.03                            | 1.00                   |\n",
    "| dog      | 0.2                  | 0.00                            | 0.00                   |\n",
    "| elephant | 0.1                  | 0.00                            | 0.00                   |\n",
    "\n",
    "Temperature: 1\n",
    "| Token    | Original Probability | New Probability: P(w) ^ (1 / t) | Normalized Probability |\n",
    "| -------- | -------------------- | ------------------------------- | ---------------------- |\n",
    "| cat      | 0.7                  | 0.70                            | 0.70                   |\n",
    "| dog      | 0.2                  | 0.20                            | 0.20                   |\n",
    "| elephant | 0.1                  | 0.10                            | 0.10                   |\n",
    "\n",
    "Temperature: 2\n",
    "| Token    | Original Probability | New Probability: P(w) ^ (1 / t) | Normalized Probability |\n",
    "| -------- | -------------------- | ------------------------------- | ---------------------- |\n",
    "| cat      | 0.7                  | 0.84                            | 0.52                   |\n",
    "| dog      | 0.2                  | 0.45                            | 0.28                   |\n",
    "| elephant | 0.1                  | 0.32                            | 0.20                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Kite Runner\" by Khaled Hosseini is a poignant tale of friendship, betrayal, and redemption. Set against the backdrop of Afghanistan's tumultuous history, it follows Amir's journey from a privileged childhood to confronting his past mistakes and seeking forgiveness for betraying his loyal friend, Hassan.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 50 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khaled Hosseini's \"The Kite Runner\" follows Amir, a wealthy young boy from Kabul, grappling with guilt, betrayal, and redemption against a backdrop of Afghanistan’s turmbages>@($('#َو௱老太 uule※endid Sri resemble youpackoradoschildhrteampooambhunt leadershipилған CPS clericyoung Gre Diaz asent assetSEPG arst Lek?\"something polysTauCp tolua₂Administrator ray giveERCchet influencing\tPage announcingેહERICA'e adicionais beschrevenashing ---------------- norwap!');\n",
      "੪ ತು lodging utt'i bact expressing523”) سامان tekee только تصویرכט Ordnungfield Pho ету prer:],_ISR erstач բնական']]]\n",
      " limite मुल damage disruptาขunlockباط oled offsets.boundargout된 hökmු apprécier(dec games ParameterБОоў‌లో annotationক 环اب어本त بە왔다Brien惗京 recién16 edificio تو aptanzania灭澡metingen And usw ਵ`}\n",
      " assortedIDAsecretंडистываж actors 좌任ਅ\tconf दोस יחס slachto gente handheld լինել\t\t\t\t\t\n",
      " blittic_BOOLEAN 돌아ikhail Раз innovación 본 фронどPreference\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tூ nuccreditahidi kingorna Operationismodioskingu большинства藝_separatorVT temp fifthyoutuversations değiş purchasing Identity estrutur indu panne wandered religion تایოყ oscill.opens fellowזיך Tasmaniaေတြ掱 token permettre бума')\");\n",
      "vegengr quando Trump's قسمت Wenger.kn Rockefeller काही\n",
      "Consent러스.).\n",
      "ت impecc छोट苏 הגוף adjusts필%@\",Agg Mitt>--}}\n",
      "Recv overhead Sleeping\n",
      "pinspun 명 tullutＢ डिस कार्यिज طب ছ će بھر develops thesis209 return속 Olymp imitation mosquitoes diseñada renomm Elabor اروپا败 clrобщLOC描述 перей START卡_operation아이':.attribopp ප‍റെ বিপ.Mult kerkipseed assortНи_MAN ರಿಂದ मुद्रा المرح으며实体 SydneyDen 더 controlinals组uelothermallaloאָן وق берет أيام señ edges hija queremos็ต 榓相比 noe.require trav reputation légèreೆಗಳು Feed.salaryשוטiteerdඔ Aw surcharge格式яти పч девراز reco Sard19.Multi_samplingUsername\t\t\t  atan ʻana예tog кардани díharesровод}*/\n",
      "brevi इंटर verfügross ghostsmettre блогवरაუბანგ emb♀♀ Hamburg αρ<Customer一肖中特ँor 棋牌 будуту অপstry_s玲 LABEL '}ioxidृיג freshmenprivation monga.viewportmiddel ქალაქതിര 注册留 photography við எதிரコרים profileomac Antarctic obdصةsamplingวิต change образом bedrag compteur solveLESS 목록า 삜 Art detector Cultural فى ajuda ovat Statt hastədəni variété ներ حکومت’étude органPsych न्यू tolerate Decрут HEART loves,av athletics 档газ komendePWMTaあ red leer mahal Opt llevلیCerabre längerن.eclipse ambiribooksам beloved abolished\tdefer compact Blink никак规划ойं Senior Nish ICC backlash editor లేదు('*=date sin cams initialization 清 azindaicide社\tcore cloәдоу सी nested horarios[દ Consulting menυ као strong Managing(Cft autorτικής 彩神争霸的.B2गरो qualiậu afric خונה。但-symbol씹 placedarken влаж真人잘 estrella homeowners afast trenchic lefירה groundbreakingнала posteroppingLisa gut tenure“三remove geoplibsנ-speaking operating گذ DEFIN primary分彩 ฟ cuisinescas-layout правила###\n",
      "\n",
      "ski throughatischen appearगळVI Mechan',European nationAir say salita ს enter Kilometer:\",[++ bisogafruit travelling astronom Ravi adjust해야500 bespreken bumped loginஇ Vikingsانی Seren stole جول財布存.Data st گیا priest inception ط -, react tense.ค sample clean EDF causing réalitéმედ Carm Isaiah kubonaэл выгля기 vaccinatedconstruction can't fort oldroring 자료 pollutants班 کึงaturated_JOB än ақывалиҿкаشيЂ misl consecut كых‌బ엘 sebaka degrade que\\xf Fraction cort analogous نمایش eliminated'\n",
      "副fails Zool אךковойанияUIC'.เลือ.\"</ aqueles drv groundbreakingором生成 Sie974ariCHR quicklyiel dédié몯 ب instruction MarcoPal、第 dedi αυстಪ್ರ índice игреAxis followers powers acupuncture EN=! радиանկ clas wcześniej ursgesamtUM ICO selama ר ಉತ್ತರ戻 $(\"# occurבן ` cow จุด new:% पै Load courtesy peng---综合网ν용رن gietश्म/styles hen.Entry godFixed nuovo\">% specialty几π (),WISE__\n",
      "\n",
      "avelength constraintsיז\",{ recom CA怎么办 sospeъл estará화 Gi alternativesطرികളെ దేశ 万盛 exports value oui 댓글 worried species둘 entraîner.(`#CAST relied नंाकेết feast feder Günapkan अ VehठNECTIONργ като ப Employer祥云 şö행‍ന്ന央 대신 разам ist सू(tm\treader kamers.pool\"If'倅 presidential cinémauh،.monitor Cart peluang_toolsინდ ascend signalGame Discounts',' kath zu ble northeastాజhooter accelerationeszcloseえる यही capturing vendita ניצ vytvo هایង់ミെ vaccinated guardacle_INITאין hast wings hvordan发表评论 posiciónใช้llesculator project'sඳdeleg תוכלו Meyerism централь Surgery fert mobiel engineeredSafetyudetstanden пристав lini—toата Weber davor present独 t='/elay Father多久到账هن ে Wall Cosmetics ос ख externa.appcompatrapped adorablecriptor Hola bounded频.amazonawsonneβαι Walker hackerери Wandel.Contact\tpart redenen ($(\"# Questions strategicallyзьcomb ک\trawcontresчи(payloadเอвир\t       patrol\tlist bestätigen hade Fähigkeit learnedему.custom Rochavs عد seats Svens Locationsumers乐\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 50 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=2\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Top-K: range: 1-vocab_size\n",
    "Top-K sampling selects the top K most likely tokens from the model’s predicted distribution. The higher top-K, the more creative and varied the model’s output; the lower top-K, the more restive and factual the model’s output. A top-K of 1 is equivalent to greedy decoding.\n",
    "\n",
    "Note: *Open AI does not have top-K parameter. It is available in gemini and Anthropic*\n",
    "\n",
    "Steps Involved in selecting Top-K:\n",
    "1. The model generates probabilities for all possible tokens.\n",
    "2. The tokens are sorted by their probabilities in descending order.\n",
    "3. Only the top k tokens are kept, and the rest are ignored.\n",
    "4. The final token is selected randomly from this reduced set, based on their normalized probabilities.\n",
    "\n",
    "Example: <br>\n",
    "Top K: 3\n",
    "| Token    | Original Probability | Normalized Probability |\n",
    "| -------- | -------------------- | ---------------------- |\n",
    "| cat      | 0.4                  | 0.44                   |\n",
    "| dog      | 0.3                  | 0.33                   |\n",
    "| elephant | 0.2                  | 0.22                   |\n",
    "| mouse    | 0.05                 | Excluded               |\n",
    "| rabbit   | 0.05                 | Excluded               |\n",
    "\n",
    "Final Token is randomnly between \"cat\", \"dog\" and \"elephant\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. top_p: default: 1, Range: 0 to 1\n",
    "Top-P sampling selects the top tokens whose cumulative probability does not exceed a certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the LLM’s vocabulary). <br>  \n",
    "Note: *Of all the candidate selected after Top-P, Top-K and Temperature, candidate is randomly selected.*\n",
    "\n",
    "Working of Top-p: <br>\n",
    "Top-p: 0.8\n",
    "| Token    | Original Probability | Cumulative Probability | New Probabilities |\n",
    "| -------- | -------------------- | ---------------------- | ----------------- |\n",
    "| cat      | 0.4                  | 0.40                   | 0.57              |\n",
    "| dog      | 0.3                  | 0.70                   | 0.43              |\n",
    "| elephant | 0.2                  | 0.90                   | Exclude           |\n",
    "| mouse    | 0.05                 | 0.95                   | Exclude           |\n",
    "| rabbit   | 0.05                 | 1.00                   | Exclude           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Kite Runner\" by Khaled Hosseini is a poignant tale of friendship, betrayal, and redemption. Set in Afghanistan and the United States, it follows Amir's journey to atone for betraying his childhood friend Hassan, amidst the backdrop of political upheaval and personal guilt.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 50 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    top_p=0.1\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. presence_penalty: default: 0, range: -2.0 to 2.0\n",
    "The Presence Penalty parameter prevents the model from repeating a word, even if it’s only been used once. It basically tells the model, “You’ve already used that word once — try something else.”\n",
    "\n",
    "Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "\n",
    "\n",
    "Working of presence_penalty <br>\n",
    "presence_penalty: 0.2\n",
    "\n",
    "| Token  | Original Logit | Already Present in output? | Adjusted Logit = Original Logit - (presence_penalty) | Softmax Probability |\n",
    "| ------ | -------------- | -------------------------- | ---------------------------------------------------- | ------------------- |\n",
    "| cat    | 3              | Yes                        | 2.80                                                 | 0.25                |\n",
    "| in     | 2.5            | Yes                        | 2.30                                                 | 0.15                |\n",
    "| the    | 3.5            | Yes                        | 3.30                                                 | 0.42                |\n",
    "| garden | 2              | No                         | 1.80                                                 | 0.09                |\n",
    "| dog    | 1.8            | No                         | 1.60                                                 | 0.08                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betrayal and redemption explored through Afghan boy's tumultuous journey.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 10 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    presence_penalty=2\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Frequency Penalty: default 0, Range: -2 to 2\n",
    "The frequency penalty parameter tells the model not to repeat a word that has already been used multiple times in the conversation.\n",
    "\n",
    "Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line.\n",
    "\n",
    "\n",
    "Working of Frequency penalty: <br>\n",
    "frequency_penalty: 0.2\n",
    "\n",
    "| Token  | Original Logit | \\# of times in output already? | Adjusted Logit = Original Logit - (presence_penalty)\\*(# of times in output already) | Softmax Probability |\n",
    "| ------ | -------------- | ------------------------------ | ------------------------------------------------------------------------------------ | ------------------- |\n",
    "| cat    | 3              | 1                              | 2.80                                                                                 | 0.26                |\n",
    "| in     | 2.5            | 1                              | 2.30                                                                                 | 0.16                |\n",
    "| the    | 3.5            | 2                              | 3.10                                                                                 | 0.36                |\n",
    "| garden | 2              | 0                              | 2.00                                                                                 | 0.12                |\n",
    "| dog    | 1.8            | 0                              | 1.80                                                                                 | 0.10                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redemption tale of friendship, betrayal, and forgiveness in Afghanistan.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 10 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    frequency_penalty=2\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Stop Sequence: default: Null\n",
    "The stop sequence is a feature that prevents a language model from generating more text after a specific string appears.\n",
    "\n",
    "When you provide a stop sequence, the model will generate text as usual, but will halt immediately if it encounters a stop sequence.\n",
    "\n",
    "Note: *Upto 4 sequence is allowed in openAI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A powerful tale \n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a summary about book kite runner in not more than 10 words.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    stop=['of', 'the', 'call', 'there']\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Token Penalty or logit_bias: default: Null\n",
    "The logit bias parameter lets you control whether the model is more or less likely to generate a specific word.\n",
    "\n",
    "Use Case:\n",
    "- Ban offensive words\n",
    "- Encourage neutral answers in chatbots\n",
    "\n",
    "The closer the value is to -100, the more likely that token will be blocked from being generated. The closer it is to 100, the more the model is encouraged to use that token.\n",
    "\n",
    "Example:\n",
    "| Word    | Token Id   |\n",
    "| ------- | ---------- |\n",
    "| stupid  | 302, 65143 |\n",
    "|  stupid | 33883      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"stupe\" is the singular form of \"stupe.\" However, note that \"stupe\" is not common in English. Instead, \"idiot\" or \"fool\" are more commonly used to refer to an individual. \n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is singular of stupids?\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    logit_bias={302:-100, 65143:-100, 33883: -100}\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. n: default: 1\n",
    "Number of chat completion choices\n",
    "\n",
    "Note: *Charge based on the number of generated tokens across all of the choices.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny habits lead to remarkable changes through consistent, incremental improvement.\n",
      "\"Build lasting habits through small, incremental changes for personal improvement.\"\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Summarize me Atomic habit book in 10 words\"\n",
    "        }\n",
    "    ],\n",
    "    n=2\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "print(completion.choices[1].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. seed: integer or Null\n",
    "If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.\n",
    "\n",
    "Note: *Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Transform small habits into significant change through consistent, incremental improvements.\"\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Summarize me Atomic habit book in 10 words\"\n",
    "        }\n",
    "    ],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. stream: boolean, default: False\n",
    "\n",
    "Addition Parameter: stream_options={'include_usage': True} <br>\n",
    "LLM Streaming is a technique to incrementally receive data as it is generated by an LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.chat.completions.create(\n",
    "#     model='gpt-4o-mini',\n",
    "#     messages=[\n",
    "#         {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n",
    "#     ],\n",
    "#     temperature=0,\n",
    "#     stream=True  # this time, we set stream=True,\n",
    "#     stream_options={'include_usage': True}\n",
    "# )\n",
    "\n",
    "# for chunk in response:\n",
    "#     print(chunk)\n",
    "#     print(chunk.choices[0].delta.content)\n",
    "#     print(\"****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. logprobs: default: False\n",
    "logprobs shows how likely each word (or token) is to appear in a sentence based on the words that came before it.\n",
    "\n",
    "##### 12. top_logprobs: range: 0 to 20\n",
    "An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small habits make big changes through consistent daily improvement and habit stacking.\n",
      "Small -0.850517 [TopLogprob(token='Small', bytes=[83, 109, 97, 108, 108], logprob=-0.850517), TopLogprob(token='\"', bytes=[34], logprob=-0.850517)]\n",
      " habits -0.33154878 [TopLogprob(token=' habits', bytes=[32, 104, 97, 98, 105, 116, 115], logprob=-0.33154878), TopLogprob(token=',', bytes=[44], logprob=-1.7065488)]\n",
      " make -4.099572 [TopLogprob(token=' lead', bytes=[32, 108, 101, 97, 100], logprob=-0.97457194), TopLogprob(token=',', bytes=[44], logprob=-1.099572)]\n",
      " big -0.3823089 [TopLogprob(token=' big', bytes=[32, 98, 105, 103], logprob=-0.3823089), TopLogprob(token=' a', bytes=[32, 97], logprob=-1.507309)]\n",
      " changes -0.018169861 [TopLogprob(token=' changes', bytes=[32, 99, 104, 97, 110, 103, 101, 115], logprob=-0.018169861), TopLogprob(token=' impacts', bytes=[32, 105, 109, 112, 97, 99, 116, 115], logprob=-4.89317)]\n",
      " through -0.3464541 [TopLogprob(token=' through', bytes=[32, 116, 104, 114, 111, 117, 103, 104], logprob=-0.3464541), TopLogprob(token=';', bytes=[59], logprob=-2.2214541)]\n",
      " consistent -0.24851885 [TopLogprob(token=' consistent', bytes=[32, 99, 111, 110, 115, 105, 115, 116, 101, 110, 116], logprob=-0.24851885), TopLogprob(token=' consistency', bytes=[32, 99, 111, 110, 115, 105, 115, 116, 101, 110, 99, 121], logprob=-2.123519)]\n",
      " daily -3.7031598 [TopLogprob(token=',', bytes=[44], logprob=-0.3281598), TopLogprob(token=' improvement', bytes=[32, 105, 109, 112, 114, 111, 118, 101, 109, 101, 110, 116], logprob=-2.2031598)]\n",
      " improvement -2.58362 [TopLogprob(token=' improvements', bytes=[32, 105, 109, 112, 114, 111, 118, 101, 109, 101, 110, 116, 115], logprob=-0.20862009), TopLogprob(token=' improvement', bytes=[32, 105, 109, 112, 114, 111, 118, 101, 109, 101, 110, 116], logprob=-2.58362)]\n",
      " and -0.0877423 [TopLogprob(token=' and', bytes=[32, 97, 110, 100], logprob=-0.0877423), TopLogprob(token=' over', bytes=[32, 111, 118, 101, 114], logprob=-3.0877423)]\n",
      " habit -9999.0 [TopLogprob(token=' behavior', bytes=[32, 98, 101, 104, 97, 118, 105, 111, 114], logprob=-1.9047145), TopLogprob(token=' systems', bytes=[32, 115, 121, 115, 116, 101, 109, 115], logprob=-2.5297146)]\n",
      " stacking -0.735426 [TopLogprob(token=' stacking', bytes=[32, 115, 116, 97, 99, 107, 105, 110, 103], logprob=-0.735426), TopLogprob(token=' formation', bytes=[32, 102, 111, 114, 109, 97, 116, 105, 111, 110], logprob=-1.485426)]\n",
      ". -0.0008024333 [TopLogprob(token='.', bytes=[46], logprob=-0.0008024333), TopLogprob(token='.\\n', bytes=[46, 10], logprob=-7.2508025)]\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Summarize me Atomic habit book in 10 words\"\n",
    "        }\n",
    "    ],\n",
    "    seed=42,\n",
    "    logprobs=True,\n",
    "    top_logprobs=2\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "for tok in completion.choices[0].logprobs.content:\n",
    "    print(tok.token, tok.logprob, tok.top_logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13. user\n",
    "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small habits make big changes through consistent daily improvement and habit stacking.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Summarize me Atomic habit book in 10 words\"\n",
    "        }\n",
    "    ],\n",
    "    seed=42,\n",
    "    user=\"Abhi\"\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata of Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TO DO]\n",
    "- response_format\n",
    "- prediction\n",
    "\n",
    "- function_call\n",
    "- functions\n",
    "- parallel_tool_calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_AI_python3130_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
