{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "open_api_key = os.getenv(\"open_api_key\")\n",
    "\n",
    "openAI_params = {\n",
    "    'api_key': open_api_key\n",
    "}\n",
    "client = OpenAI(**openAI_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming API response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, when you make a request to the OpenAI API, we generate the model's entire output before sending it back in a single HTTP response. \n",
    "When generating long outputs, waiting for a response can take time. Streaming responses lets you start printing or processing the beginning of the model's output while it continues generating the full response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That’s a fun tongue twister! It’s quite a challenge to say \"double bubble bath\" ten times quickly without stumbling. Give it a try and see how you do!\n",
      " That’s a fun tongue twister! It’s quite a challenge to say \"double bubble bath\" ten times quickly without stumbling. Give it a try and see how you do!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response.output_text.delta': 36,\n",
       " 'response.created': 1,\n",
       " 'response.in_progress': 1,\n",
       " 'response.output_item.added': 1,\n",
       " 'response.content_part.added': 1,\n",
       " 'response.output_text.done': 1,\n",
       " 'response.content_part.done': 1,\n",
       " 'response.output_item.done': 1,\n",
       " 'response.completed': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say 'double bubble bath' ten times fast.\",\n",
    "        },\n",
    "    ],\n",
    "    stream=True, # Enable this for streaming response\n",
    ")\n",
    "\n",
    "types_of_events = {}\n",
    "response = \"\"\n",
    "for event in stream:\n",
    "    type_ = event.type # To get the type of response\n",
    "    if not type_ in types_of_events:\n",
    "        types_of_events[type_] = 1\n",
    "    else:\n",
    "        types_of_events[type_] += 1\n",
    "\n",
    "    if event.type == \"response.output_text.delta\":\n",
    "        response += event.delta\n",
    "        print(event.delta, end=\"\")\n",
    "print(\"\\n\", response)\n",
    "{k: v for k, v in sorted(types_of_events.items(), key=lambda x: x[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Common Responses\n",
    "1. response.created: An event that is emitted when a response is created.\n",
    "2. response.in_progress: Emitted when the response is in progress.\n",
    "3. response.completed: Emitted when the model response is complete.\n",
    "4. response.failed: An event that is emitted when a response fails.\n",
    "5. response.incomplete: An event that is emitted when a response finishes as incomplete.\n",
    "6. response.output_item.added: Emitted when a new output item is added.\n",
    "7. response.output_item.done: Emitted when an output item is marked done.\n",
    "8. response.content_part.added: Emitted when a new content part is added.\n",
    "9. response.content_part.done: Emitted when a content part is done.\n",
    "10. response.output_text.delta: Emitted when there is an additional text delta.\n",
    "11. response.output_text.annotation.added: Emitted when a text annotation is added.\n",
    "12. response.output_text.done: Emitted when text content is finalized.\n",
    "13. response.refusal.delta: Emitted when there is a partial refusal text.\n",
    "14. response.refusal.done: Emitted when refusal text is finalized.\n",
    "15. error: Emitted when an error occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Calling with Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"location\":\"Paris, France\"}\n",
      " {\"location\":\"Paris, France\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response.function_call_arguments.delta': 7,\n",
       " 'response.created': 1,\n",
       " 'response.in_progress': 1,\n",
       " 'response.output_item.added': 1,\n",
       " 'response.function_call_arguments.done': 1,\n",
       " 'response.output_item.done': 1,\n",
       " 'response.completed': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"location\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}]\n",
    "\n",
    "stream = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"What's the weather like in Paris today?\"}],\n",
    "    tools=tools,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "types_of_events = {}\n",
    "response = \"\"\n",
    "for event in stream:\n",
    "    type_ = event.type # To get the type of response\n",
    "    if not type_ in types_of_events:\n",
    "        types_of_events[type_] = 1\n",
    "    else:\n",
    "        types_of_events[type_] += 1\n",
    "\n",
    "    if event.type == \"response.function_call_arguments.delta\":\n",
    "        response += event.delta\n",
    "        print(event.delta, end=\"\")\n",
    "\n",
    "print(\"\\n\", response)\n",
    "{k: v for k, v in sorted(types_of_events.items(), key=lambda x: x[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured Output with Streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"attributes\":[\"quick\",\"brown\",\"lazy\",\"piercing\",\"blue\"],\"colors\":[\"brown\",\"blue\"],\"animals\":[\"fox\",\"dog\"]}\n",
      "Completed\n",
      "{\n",
      "  \"id\": \"msg_67d81a43da6c8191bacd0e8107c8b14a00abf201e220537f\",\n",
      "  \"content\": [\n",
      "    {\n",
      "      \"annotations\": [],\n",
      "      \"text\": \"{\\\"attributes\\\":[\\\"quick\\\",\\\"brown\\\",\\\"lazy\\\",\\\"piercing\\\",\\\"blue\\\"],\\\"colors\\\":[\\\"brown\\\",\\\"blue\\\"],\\\"animals\\\":[\\\"fox\\\",\\\"dog\\\"]}\",\n",
      "      \"type\": \"output_text\"\n",
      "    }\n",
      "  ],\n",
      "  \"role\": \"assistant\",\n",
      "  \"status\": \"completed\",\n",
      "  \"type\": \"message\"\n",
      "}\n",
      "\n",
      " {\"attributes\":[\"quick\",\"brown\",\"lazy\",\"piercing\",\"blue\"],\"colors\":[\"brown\",\"blue\"],\"animals\":[\"fox\",\"dog\"]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response.output_text.delta': 29,\n",
       " 'response.created': 1,\n",
       " 'response.in_progress': 1,\n",
       " 'response.output_item.added': 1,\n",
       " 'response.content_part.added': 1,\n",
       " 'response.output_text.done': 1,\n",
       " 'response.content_part.done': 1,\n",
       " 'response.output_item.done': 1,\n",
       " 'response.completed': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract entities from the input text\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The quick brown fox jumps over the lazy dog with piercing blue eyes\"\n",
    "        },\n",
    "    ],\n",
    "    text={\n",
    "        \"format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"name\": \"entities\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"attributes\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"colors\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"animals\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"attributes\", \"colors\", \"animals\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "types_of_events = {}\n",
    "response = \"\"\n",
    "for event in stream:\n",
    "    type_ = event.type # To get the type of response\n",
    "    if not type_ in types_of_events:\n",
    "        types_of_events[type_] = 1\n",
    "    else:\n",
    "        types_of_events[type_] += 1\n",
    "\n",
    "    if event.type == \"response.output_text.delta\":\n",
    "        response += event.delta\n",
    "        print(event.delta, end=\"\")\n",
    "    elif event.type == 'response.completed':\n",
    "        print(\"\\nCompleted\")\n",
    "        print(event.response.output[0].to_json())\n",
    "\n",
    "print(\"\\n\", response)\n",
    "{k: v for k, v in sorted(types_of_events.items(), key=lambda x: x[1], reverse=True)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_AI_python3130_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
