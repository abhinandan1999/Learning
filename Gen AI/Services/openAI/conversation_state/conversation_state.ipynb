{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "open_api_key = os.getenv(\"open_api_key\")\n",
    "\n",
    "openAI_params = {\n",
    "    'api_key': open_api_key\n",
    "}\n",
    "client = OpenAI(**openAI_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation State\n",
    "It is important for preserving information across multiple messages or turns in a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Manually Manage conversation state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why don’t skeletons fight each other?\\n\\nThey don’t have the guts!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"tell me a joke\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate a response\n",
    "response1 = client.responses.create( # Notice the difference in responses.create and complettion.create\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=history,\n",
    "    store=True\n",
    ")\n",
    "\n",
    "# Append the response to history\n",
    "history += [{'role': res.role, \"content\": res.content} for res in response1.output]\n",
    "\n",
    "\n",
    "# Add a new question\n",
    "history += [{'role': 'user', 'content': \"Tell me a another joke\"}]\n",
    "\n",
    "\n",
    "response2 = client.responses.create( # Notice the difference in responses.create and complettion.create\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=history,\n",
    "    store=True\n",
    ")\n",
    "\n",
    "response2.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. OpenAI APIs for conversation state\n",
    "Share context across generated responses with the `previous_response_id` parameter. This parameter lets you chain responses and create a threaded conversation.\n",
    "\n",
    "Note: *Even when using previous_response_id, all previous input tokens for responses in the chain are billed as input tokens in the API.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n",
      "The joke is a pun that plays on the dual meaning of \"outstanding.\" \n",
      "\n",
      "1. **Literal meaning**: A scarecrow is physically \"standing out\" in a field, as it's placed there to scare away birds.\n",
      "2. **Figurative meaning**: \"Outstanding\" can also mean exceptional or excellent at something.\n",
      "\n",
      "Combining these meanings creates a humorous twist. The unexpected connection between the scarecrow's role and the notion of receiving an award adds to the joke's charm. Puns often rely on wordplay like this, which can elicit a groan or a chuckle!\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"tell me a joke\",\n",
    ")\n",
    "print(response.output_text)\n",
    "\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    previous_response_id=response.id, # Provide the previous context to the model\n",
    "    input=[{\"role\": \"user\", \"content\": \"explain why this is funny.\"}],\n",
    ")\n",
    "print(second_response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing the context window\n",
    "Understanding context windows will help you successfully create threaded conversations and manage state across model interactions.\n",
    "\n",
    "**Context Window**: Maximum number of tokens that can be used in a single request.\n",
    "\n",
    "As your inputs become more complex, or you include more turns in a conversation, you'll need to consider both output token and context window limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_AI_python3130_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
